# Purpose: Module for reading DAS data.
# Author: Minzhe Hu
# Date: 2025.9.15
# Email: hmz2018@mail.ustc.edu.cn
# Partially modified from
# https://github.com/RobbinLuo/das-toolkit/blob/main/DasTools/DasPrep.py
import warnings
import json
import pickle
import numpy as np
import h5py
import segyio
from typing import Union
from pathlib import Path
from nptdms import TdmsFile
from daspy.core.util import _device_standardized_name, _h5_file_format, \
    _trimming_slice_metadata
from daspy.core.section import Section
from daspy.core.dasdatetime import DASDateTime, utc


def read(fname=None, output_type='section', ftype=None, file_format='auto',
         headonly=False, dtype=None, **kwargs) -> Union[Section, tuple]:
    """
    Read a .pkl/.pickle, .h5/.hdf5, .tdms, or .segy/.sgy file.

    :param fname: Path of DAS data file.
    :type fname: str or pathlib.PosixPath
    :param output_type: Output type, 'Section' for daspy.Section instance,
        'array' for numpy array and metadata dict.
    :type output_type: str
    :param ftype: File type or function for reading data.
    :type ftype: None, str or function
    :param file_format: Format in which the file is saved. Function is allowed
        to extract dataset and metadata.
    :type file_format: str or function
    :param headonly: If True, only metadata will be read.
    :type headonly: bool
    :param dtype: Data type of the returned data.
    :type dtype: str
    :param chmin: Minimum channel number.
    :type chmin: int
    :param chmax: Maximum channel number.
    :type chmax: int
    :param dch: Channel step.
    :type dch: int
    :param xmin: Minimum distance.
    :type xmin: float
    :param xmax: Maximum distance.
    :type xmax: float
    :param tmin: Minimum time.
    :type tmin: float or DASDateTime
    :param tmax: Maximum time.
    :type tmax: float or DASDateTime
    :param spmin: Minimum sampling point.
    :type spmin: int
    :param spmax: Maximum sampling point.
    :type spmax: int
    :return: daspy.Section instance or tuple of numpy array and metadata dict.
    :rtype: Section or tuple
    """
    fun_map = {'pkl': _read_pkl, 'h5': _read_h5, 'tdms': _read_tdms,
               'sgy': _read_segy, 'npy': _read_npy}
    if fname is None:
        fname = Path(__file__).parent / 'example.pkl'
        ftype = 'pkl'
    elif ftype is None:
        ftype = str(fname).split('.')[-1].lower()

    if 'ch1' in kwargs.keys() or 'ch2' in kwargs.keys():
        warnings.warn("In future versions, parameter 'ch1' and 'ch2' will be "
                      "replaced by 'chmin' and 'chmax'", FutureWarning)
        kwargs['chmin'] = kwargs.pop('ch1', None)
        kwargs['chmax'] = kwargs.pop('ch2', None)
    if callable(ftype):
        try:
            data, metadata = ftype(fname, headonly=headonly, **kwargs)
        except TypeError:
            data, metadata = ftype(fname)
            si, sj, metadata = _trimming_slice_metadata(data.shape,
                metadata=metadata, **kwargs)
            data = data[si, sj]
    else:
        for rtp in [('pickle', 'pkl'), ('hdf5', 'h5'), ('segy', 'sgy')]:
            ftype = ftype.replace(*rtp)
        data, metadata = fun_map[ftype](fname, headonly=headonly,
                                        file_format=file_format, **kwargs)

    if dtype is not None:
        data = data.astype(dtype)
    if output_type.lower() == 'section':
        metadata['source'] = Path(fname)
        metadata['source_type'] = ftype
        data[np.isnan(data)] = 0
        return Section(data, **metadata)
    elif output_type.lower() == 'array':
        return data, metadata


def _read_pkl(fname, headonly=False, file_format='auto', chmin=None, chmax=None,
              dch=1, xmin=None, xmax=None, tmin=None, tmax=None, spmin=None,
              spmax=None):
    """
    Read data and metadata from a pickle file.
    """
    with open(fname, 'rb') as f:
        pkl_data = pickle.load(f)
        if isinstance(pkl_data, np.ndarray):
            warnings.warn('This data format doesn\'t include channel interval'
                          'and sampling rate. Please set manually')
            si, sj, metadata = _trimming_slice_metadata(pkl_data.shape,
                chmin=chmin, chmax=chmax, dch=dch, xmin=xmin, xmax=xmax,
                tmin=tmin, tmax=tmax, spmin=spmin, spmax=spmax)
            if headonly:
                data = np.zeros_like(pkl_data[si, sj])
            else:
                data = pkl_data[si, sj]
            return data, metadata
        elif isinstance(pkl_data, dict):
            data = pkl_data.pop('data')
            si, sj, metadata = _trimming_slice_metadata(data.shape,
                metadata=pkl_data, chmin=chmin, chmax=chmax, dch=dch,
                xmin=xmin, xmax=xmax, tmin=tmin, tmax=tmax, spmin=spmin,
                spmax=spmax)
            if headonly:
                data = np.zeros_like(data[si, sj])
            else:
                data = data[si, sj]
            return data, metadata
        else:
            raise TypeError('Unknown data type.')


def _read_h5_headers(group):
    """
    Recursively read HDF5 group attributes and headers.
    """ 
    headers = {}
    if len(group.attrs) != 0:
        headers['attrs'] = dict(group.attrs)
    if isinstance(group, h5py._hl.dataset.Dataset):
        return headers
    for key, value in group.items():
        try:
            gp_headers = _read_h5_headers(value)
        except AttributeError:
            headers[key] = value
        if len(gp_headers):
            headers[key] = gp_headers

    return headers


def _read_h5(fname, headonly=False, file_format='auto', chmin=None, chmax=None,
             dch=1, xmin=None, xmax=None, tmin=None, tmax=None, spmin=None,
             spmax=None):
    """
    Read data and metadata from an HDF5 file.
    """
    with h5py.File(fname, 'r') as h5_file:
        keys = h5_file.keys()
        group = list(keys)[0]
        if file_format == 'auto':
            file_format = _h5_file_format(h5_file)
        elif isinstance(file_format, str):
            file_format = _device_standardized_name(file_format)
        transpose = False
        if callable(file_format):
            dataset, transpose, metadata = file_format(h5_file)
        elif file_format == 'AP Sensing':
            dataset = h5_file['strain']
            transpose = True
            metadata = {'dx': h5_file['spatialsampling'][()],
                        'fs': h5_file['RepetitionFrequency'][()],
                        'gauge_length': h5_file['GaugeLength'][()]}

        elif file_format == 'Arag√≥n Photonics HDAS':
            for key in keys:
                if 'data' in key.lower():
                    data_key = key
                elif 'header' in key.lower():
                    header_key = key
            dataset = h5_file[data_key]
            header = h5_file[header_key][()]
            if header.ndim == 2:
                header = header[0]
            if 'start_time' in h5_file[data_key].attrs.keys():
                start_time = DASDateTime.fromisoformat(h5_file[data_key].
                                                       attrs['start_time'])
            else:
                start_time = DASDateTime.fromtimestamp(float(header[100])).utc()
                transpose = True
            metadata = {'dx': header[1],
                        'fs': header[6] / header[15] / header[98],
                        'start_distance': header[11],
                        'start_time': start_time}

        elif file_format == 'ASN OptoDAS': # https://github.com/ASN-Norway/simpleDAS
            dataset = h5_file['data']
            metadata = {'dx': h5_file['header/dx'][()],
                        'fs': 1 / h5_file['header/dt'][()],
                        'start_time': DASDateTime.fromtimestamp(
                            h5_file['header/time'][()]).utc(),
                        'scale': h5_file['header/dataScale'][()]}
            if h5_file['header/gaugeLength'][()] != np.nan:
                metadata['gauge_length'] = h5_file['header/gaugeLength'][()]
            if h5_file['header/dimensionNames'][0] == b'time':
                transpose = True

        elif file_format in ['Febus A1-R', 'Febus A1']:
            acquisition = list(h5_file[f'{group}/Source1/Zone1'].keys())[0]
            dataset = h5_file[f'{group}/Source1/Zone1/{acquisition}']
            transpose = True
            attrs = h5_file[f'{group}/Source1/Zone1'].attrs
            try:
                fs = float(attrs['FreqRes'])
            except KeyError:
                try:
                    fs = (attrs['PulseRateFreq'][0] /
                            attrs['SamplingRes'][0]) / 1000
                except KeyError:
                    try:
                        fs = 1000 / attrs['Spacing'][1]
                    except KeyError:
                        fs = attrs['SamplingRate'][0]
            time = h5_file[f'{group}/Source1/time']
            if len(time.shape) == 2: # Febus A1-R
                start_time = DASDateTime.fromtimestamp(time[0, 0]).utc()
            elif len(time.shape) == 1: # Febus A1
                start_time = DASDateTime.fromtimestamp(time[0]).utc()
            metadata = {'dx': attrs['Spacing'][0], 'fs': fs,
                        'start_channel': int(attrs['Extent'][0]),
                        'start_distance': attrs['Origin'][0],
                        'start_time': start_time,
                        'gauge_length': attrs['GaugeLength'][0]}

        elif file_format == 'OptaSense ODH3':
            dataset = h5_file['data']
            dx = (h5_file['x_axis'][-1] - h5_file['x_axis'][0]) / \
                (len(h5_file['x_axis']) - 1)
            fs = (len(h5_file['t_axis']) - 1) / (h5_file['t_axis'][-1] -
                                                 h5_file['t_axis'][0])
            metadata = {'dx': dx, 'fs': fs, 'start_time': h5_file['t_axis'][0]}

        elif file_format == 'OptaSense ODH4':
            dataset = h5_file['raw_data']
            attrs = h5_file.attrs
            dx = attrs['channel spacing m']
            start_channel = attrs['channel_start']
            metadata = {'dx': dx, 'fs': attrs['sampling rate Hz'],
                        'start_channel': start_channel,
                        'start_distance': start_channel * dx,
                        'start_time': DASDateTime.fromisoformat(
                            attrs['starttime']) ,
                        'data_type': attrs['raw_data_units'],
                        'scale': attrs['scale factor to strain']}

        elif file_format in ['OptaSense ODH4+', 'OptaSense QuantX',
                             'Silixa iDAS-MG', 'Sintela Onyx v1.0',
                             'Smart Earth ZD-DAS', 'Unknown']:
            dataset = h5_file['Acquisition/Raw[0]/RawData/']
            attrs = h5_file['Acquisition'].attrs
            try:
                if dataset.shape[0] != attrs['NumberOfLoci']:
                    transpose = True
            except KeyError:
                pass

            try:
                fs = h5_file['Acquisition/Raw[0]'].attrs['OutputDataRate']
            except KeyError:
                time_arr = h5_file['Acquisition/Raw[0]/RawDataTime/']
                fs = 1 / (np.diff(time_arr).mean() / 1e6)

            try:
                stime = dataset.attrs['PartStartTime']
            except KeyError:
                try:
                    stime = attrs['MeasurementStartTime']
                except KeyError:
                    try:
                        stime = h5_file['Acquisition/Raw[0]/RawDataTime/'][0]
                    except KeyError:
                        stime = 0
                        
            if isinstance(stime, bytes):
                stime = stime.decode('ascii')

            if isinstance(stime, str):
                stime = DASDateTime.fromisoformat(stime)
            else:
                stime = DASDateTime.fromtimestamp(stime / 1e6).astimezone(utc)

            metadata = {'dx': attrs['SpatialSamplingInterval'], 'fs': fs,
                        'start_time': stime,
                        'gauge_length': attrs['GaugeLength']}

        elif file_format == 'Silixa iDAS':
            dataset = h5_file['Acquisition/Raw[0]/RawData/']
            attrs = h5_file['Acquisition/Raw[0]'].attrs
            if dataset.shape[0] != attrs['NumberOfLoci']:
                transpose = True

            dx = np.mean(h5_file['Mapping/MeasuredSpatialResolution'])
            start_distance = h5_file['Acquisition/Custom/UserSettings'].\
                    attrs['StartDistance']
            start_time = DASDateTime.fromisoformat(
                h5_file['Acquisition/Raw[0]/RawData'].attrs['PartStartTime'].
                decode('ascii'))

            metadata = {'dx': dx, 'fs': attrs['OutputDataRate'],
                        'start_distance': start_distance,
                        'start_time':start_time,
                        'gauge_length': h5_file['Acquisition'].
                            attrs['GaugeLength'],
                        'geometry':  np.vstack((h5_file['Mapping/Lon'],
                                                h5_file['Mapping/Lat'])).T,
                        'scale': attrs['AmpScaling']}

        elif file_format == 'T8 Sensor':
            ds_name = list(h5_file['/ProcessedData'].keys())[0]
            dpath = f'/ProcessedData/{ds_name}'
            dataset = h5_file[dpath]
            transpose = True
            attrs = h5_file[dpath].attrs
            dx = float(attrs['record/channel_spacing_m']) * float(
                attrs['downsampling/decimation_in_length'])
            fs = float(attrs['record/pulse_repetition_rate_hz']) / float(
                attrs['downsampling/decimation_in_time'])
            metadata = {'dx': dx, 'fs':fs,
                        'start_distance': float(attrs['record/line_offset_m']),
                        'start_time': DASDateTime.strptime(
                            attrs['record/start_time'], '%Y%m%dT%H%M%S%f'),
                        'gauge_length': float(attrs['record/gauge_length_m'])}
            if 'event/time' in attrs.keys():
                metadata['origin_time'] = DASDateTime.strptime(
                    attrs['event/time'], '%Y%m%dT%H%M%S%f')

        elif file_format == 'INGV':
            dataset = h5_file['Fiber']
            transpose = True
            # dx = h5_file.attrs['MeasureLength[m]'][0] / \
            #     len(h5_file['ChannelMap'])
            dx = (h5_file.attrs['Stop Distance (m)'][0] -
                  h5_file.attrs['Start Distance (m)'][0]) / \
                    len(h5_file['ChannelMap'])
            start_channel = int(np.argmax(h5_file['ChannelMap'][()] >= 0))
            gauge_length = h5_file.attrs['GaugeLength'][0]
            scale = 116e-9 * h5_file.attrs['SamplingFrequency[Hz]'] / \
                gauge_length / 8192 / h5_file.attrs['FilterGain']
            metadata = {'dx': dx, 'fs': h5_file.attrs['Samplerate'][0],
                        'start_channel': start_channel,
                        'start_distance': h5_file.attrs['Start Distance (m)']
                            [0] + start_channel * dx,
                        'start_time': DASDateTime.fromtimestamp(h5_file.attrs
                            ['StartTime'][0] / 1e6).utc(),
                        'gauge_length': gauge_length, 'scale': scale}

        elif file_format in 'JAMSTEC':
            dataset = h5_file['DAS_record']
            metadata = {'dx': h5_file['Sampling_interval_in_space'][0],
                        'fs': 1 / h5_file['Sampling_interval_in_time'][0]}

        elif file_format == 'FORESEE':
            dataset = h5_file['raw']
            fs = round(1 / np.diff(h5_file['timestamp']).mean())
            start_time = DASDateTime.fromtimestamp(
                h5_file['timestamp'][0]).astimezone(utc)
            warnings.warn('This data format doesn\'t include channel interval. '
                            'Please set manually')
            metadata = {'dx': None, 'fs': fs, 'start_time': start_time}

        elif file_format == 'AI4EPS': # https://ai4eps.github.io/homepage/ml4earth/seismic_event_format_das/
            dataset = h5_file['data']
            attr = h5_file['data'].attrs
            dx = attr['dx_m']
            metadata = {'dx': dx, 'fs': 1 / attr['dt_s'],
                        'start_time': DASDateTime.fromisoformat(
                            attr['begin_time']),
                        'data_type': attr['unit']}
            if 'event_time' in attr.keys():
                metadata['origin_time'] = DASDateTime.fromisoformat(
                    attr['event_time'])

        elif file_format == 'Unknown0':
            dataset = h5_file['data_product/data']
            nch = h5_file.attrs['nx']
            if h5_file['data_product/data'].shape[0] != nch:
                transpose = True

            if h5_file.attrs['saving_start_gps_time'] > 0:
                start_time = DASDateTime.fromtimestamp(
                    h5_file.attrs['file_start_gps_time'])
            else:
                start_time = DASDateTime.fromtimestamp(
                    h5_file.attrs['file_start_computer_time'])
            metadata = {'dx': h5_file.attrs['dx'],
                        'fs': 1 / h5_file.attrs['dt_computer'],
                        'start_time': start_time.astimezone(utc),
                        'gauge_length': h5_file.attrs['gauge_length'],
                        'data_type': h5_file.attrs['data_product']}

        metadata['file_format'] = file_format
        metadata['headers'] = _read_h5_headers(h5_file)
        shape = dataset.shape
        if len(shape) == 3:
            shape = (shape[0] * shape[1], shape[2])
        if transpose:
            shape = shape[::-1]
        si, sj, metadata = _trimming_slice_metadata(shape, metadata=metadata,
            chmin=chmin, chmax=chmax, dch=dch, xmin=xmin, xmax=xmax, tmin=tmin,
            tmax=tmax, spmin=spmin, spmax=spmax)
        if headonly:
            data = np.zeros(shape, dtype=dataset.dtype)[si, sj]
        elif transpose:
            if len(dataset.shape) == 3:
                fs = int(metadata['fs'])
                j0, k0 = divmod(sj.start, fs)
                j1, k1 = divmod(sj.stop, fs)
                k1 = (j1 - j0) * fs + k1
                j1 += 1
                data = dataset[j0:j1, :, si]
                data = data.reshape((-1, data.shape[-1]))[k0:k1, :].T
            else:
                data = dataset[sj, si].T
        else:
            data = dataset[si, sj]
    return data, metadata


def _read_tdms(fname, headonly=False, file_format='auto', chmin=None,
               chmax=None, dch=1, xmin=None, xmax=None, tmin=None, tmax=None,
               spmin=None, spmax=None):
    """
    Read data and metadata from a TDMS file. see
    https://nptdms.readthedocs.io/en/stable/quickstart.html.
    """
    with TdmsFile.read(fname) as tdms_file:
        if file_format == 'auto':
            group_name = [group.name for group in tdms_file.groups()]
            if group_name == ['Measurement']:
                key = 'Measurement'
                properties = tdms_file.properties
                version = float(properties['iDASVersion'][:3])
                if version < 2.3:
                    file_format = 'Silixa iDAS'
                elif 2.3 <= version < 2.6:
                    file_format = 'Silixa iDAS-v2'
                elif version >= 2.6:
                    file_format = 'Silixa iDAS-v3'
            elif group_name == ['DAS']:
                key = 'DAS'
                properties = tdms_file[key].properties
                file_format = 'Institute of Semiconductors, CAS'
            else:
                key = group_name[0]
                file_format = 'Unknown'
        else:
            file_format = _device_standardized_name(file_format)

        if file_format in ['Silixa iDAS', 'Silixa iDAS-v2', 'Silixa iDAS-v3',
                           'Unknown']:
            start_channel = min([int(channel.name) for channel in
                                 tdms_file[key].channels()])
            shape = (len(tdms_file[key]),
                     len(tdms_file[key][str(start_channel)]))
            metadata = {'dx': properties['SpatialResolution[m]'],
                        'fs': properties['SamplingFrequency[Hz]'],
                        'start_channel': start_channel,
                        'headers': {**properties}}
            try:
                metadata['start_distance'] = properties['Start Distance (m)']
            except KeyError:
                metadata['start_distance'] = properties['StartPosition[m]']
            
            try:
                metadata['start_time'] = DASDateTime.fromisoformat(
                    properties['ISO8601 Timestamp'])
            except KeyError:
                start_time = 0
                for time_key in ['GPSTimeStamp', 'CPUTimeStamp', 'Trigger Time']:
                    if time_key in properties.keys():
                        if isinstance(properties[time_key], str):
                            start_time = DASDateTime.fromisoformat(
                                properties[time_key])
                        elif isinstance(properties[time_key], np.datetime64):
                            start_time = DASDateTime.from_datetime(
                                properties[time_key].item())
                        else:
                            continue
                        break
                metadata['start_time'] = start_time
            if 'GaugeLength' in properties.keys():
                metadata['gauge_length'] = properties['GaugeLength']

            si, sj, metadata = _trimming_slice_metadata(shape,
                metadata=metadata, chmin=chmin, chmax=chmax, dch=dch, xmin=xmin,
                xmax=xmax, tmin=tmin, tmax=tmax, spmin=spmin, spmax=spmax)
            if headonly:
                data = np.zeros(shape,
                    dtype=tdms_file[key][str(start_channel)].dtype)[si, sj]
            else:
                data = np.asarray([tdms_file[key][str(ch)][sj] for ch in
                                   range(si.start, si.stop, si.step)])
        elif file_format == 'Institute of Semiconductors, CAS':

            try:
                start_channel = int(properties['Initial Channel'])
            except KeyError:
                start_channel = 0
            nch = int(properties['Total Channels'])
            nsp = len(tdms_file[key].channels()[0]) // nch
            metadata = {'dx': properties['Spatial Resolution'],
                        'fs': 1 / properties['Time Base'],
                        'start_channel': start_channel,
                        'start_time': DASDateTime.from_datetime(
                            properties['Trigger Time'].item()),
                        'headers': {**tdms_file.properties, **properties}}

            si, sj, metadata = _trimming_slice_metadata((nch, nsp),
                metadata=metadata, chmin=chmin, chmax=chmax, dch=dch, xmin=xmin,
                xmax=xmax, tmin=tmin, tmax=tmax, spmin=spmin, spmax=spmax)
            if headonly:
                data = np.zeros((nch, nsp),
                    dtype=tdms_file[key].channels()[0].dtype)[si, sj]
            else:
                data = np.asarray(tdms_file[key].channels()[0][sj.start * nch,
                    sj.stop * nch]).reshape((-1, nch)).T[si]

        metadata['file_format'] = file_format
    return data, metadata


def _read_segy(fname, headonly=False, file_format='auto', chmin=None,
               chmax=None, dch=1, xmin=None, xmax=None, tmin=None, tmax=None,
               spmin=None, spmax=None):
    """
    Read data and metadata from a SEG-Y file. See
    https://github.com/equinor/segyio-notebooks/blob/master/notebooks/basic/02_segy_quicklook.ipynb.
    """
    with segyio.open(fname, ignore_geometry=True) as segy_file:
        if file_format == 'auto':
            file_format = 'Unknown'
        else:
            file_format = _device_standardized_name(file_format)
        
        metadata = {'fs': 1 / (segyio.tools.dt(segy_file) / 1e6),
                    'file_format': file_format}
        warnings.warn('This data format doesn\'t include channel interval.'
                    'Please set manually')
        
        nch = segy_file.tracecount
        nsp = segy_file.trace.raw.shape
        si, sj, metadata = _trimming_slice_metadata((nch, nsp),
            metadata=metadata, chmin=chmin, chmax=chmax, dch=dch, tmin=tmin,
            tmax=tmax, spmin=spmin, spmax=spmax)

        if headonly:
            data = np.zeros((nch, nsp), dtype=segy_file.trace.raw.dtype)[si, sj]
        else:
            data = segy_file.trace.raw[si][:, sj]
        return data, metadata


def _read_npy(fname, headonly=False, chmin=None, chmax=None, dch=1, xmin=None,
              xmax=None, tmin=None, tmax=None, spmin=None, spmax=None):
    """
    Read data from a NumPy .npy file.
    """
    data = np.load(fname)
    if headonly:
        return np.zeros_like(data), {'dx': None, 'fs': None}
    else:
        si, sj, metadata = _trimming_slice_metadata(data.shape, chmin=chmin,
            chmax=chmax, spmin=spmin, spmax=spmax)
        warnings.warn('This data format doesn\'t include channel interval and '
                    'sampling rate. Please set manually')
        return data[si, sj], metadata


def read_json(fname, output_type='dict'):
    """
    Read .json metadata file.

    See :cite:`Lai2024` for format details.

    :param fname: Path of json file.
    :type fname: str or pathlib.PosixPath
    :param output_type: Output type, 'dict' for dictionary, 'Section' for empty
        daspy.Section instance.
    :type output_type: str
    :return: Metadata dictionary or daspy.Section instance without data.
    :rtype: dict or Section
    """
    with open(fname, 'r') as fcc_file:
        headers = json.load(fcc_file)
    if output_type.lower() == 'dict':
        return headers
    elif output_type.lower() in ['section', 'sec']:
        if len(headers['Overview']['Interrogator']) > 1:
            case_type = 'Multiple interrogators, single cable'
            sec_num = len(headers['Overview']['Interrogator'])
            sec = []
            for interrogator in headers['Overview']['Interrogator']:
                nch = interrogator['Acquisition'][0]['Attributes']\
                    ['number_of_channels']
                data = np.zeros((nch, 0))
                dx = interrogator['Acquisition'][0]['Attributes']\
                    ['spatial_sampling_interval']
                fs = interrogator['Acquisition'][0]['Attributes']\
                    ['acquisition_sample_rate']
                gauge_length = interrogator['Acquisition'][0]['Attributes']\
                    ['gauge_length']
                sec.append(Section(data, dx, fs, gauge_length=gauge_length,
                                   headers=headers))
        elif len(headers['Overview']['Interrogator'][0]['Acquisition']) > 1:
            case_type = 'Active survey'
            sec_num = len(
                headers['Overview']['Interrogator'][0]['Acquisition'])
            sec = []
            for acquisition in headers['Overview']['Interrogator'][0]\
                ['Acquisition']:
                nch = acquisition['Attributes']['number_of_channels']
                data = np.zeros((nch, 0))
                dx = acquisition['Attributes']['spatial_sampling_interval']
                fs = acquisition['Attributes']['acquisition_sample_rate']
                gauge_length = acquisition['Attributes']['gauge_length']
                sec.append(Section(data, dx, fs, gauge_length=gauge_length,
                                   headers=headers))
        else:
            sec_num = 1
            if len(headers['Overview']['Cable']) > 1:
                case_type = 'Single interrogators, multiple cable'
            else:
                env = headers['Overview']['Cable'][0]['Attributes']\
                    ['cable_environment']
                if env == 'trench':
                    case_type = 'Direct buried'
                elif env == 'conduit':
                    case_type = 'Dark fiber'
                elif env in ['wireline', 'outside borehole casing']:
                    case_type = 'Borehole cable'
            nch = headers['Overview']['Interrogator'][0]['Acquisition'][0]\
                ['Attributes']['number_of_channels']
            dx = headers['Overview']['Interrogator'][0]['Acquisition'][0]\
                ['Attributes']['spatial_sampling_interval']
            fs = headers['Overview']['Interrogator'][0]['Acquisition'][0]\
                ['Attributes']['acquisition_sample_rate']
            gauge_length = headers['Overview']['Interrogator'][0]\
                ['Acquisition'][0]['Attributes']['gauge_length']
            data = np.zeros((nch, 0))
            sec = Section(data, dx, fs, gauge_length=gauge_length,
                          headers=headers)

        print(f'For case of {case_type}, create {sec_num} empty daspy.Section '
              'instance(s)')
        return sec
